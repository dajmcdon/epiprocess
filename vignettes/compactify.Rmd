---
title: Compactify to remove LOCF values
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Compactify to remove LOCF values}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Removing LOCF data to save space

We need not even store rows that look like the last observation carried forward,
as they use up extra space. Furthermore, we already apply LOCF in the
epi_archive-related functions, such that the results should be the same with or
without the rows, as long as use code does not rely on directly modifying
epi_archive's fields in a way that expects all the original records and breaks
if they are trimmed down.
 
There are three different values that can be assigned to `compactify`:

* No argument: Does not put in LOCF values, and prints the first six LOCF values
that have been omitted but could have been placed.
* `TRUE`: Does not put in LOCF values, but doesn't print anything relating
to which values have been omitted.
* `FALSE`: Includes LOCF values.

For this example, we have one `epi_archive` using LOCF values, while another
doesn't use them to illustrate LOCF. LOCF can mar the performance of dataset
operations. Note that as the column
`case_rate_7d_av` has many more LOCF values than `percent_cli`, we will omit the
`percent_cli` column for comparing performance.

```{r}
library(dplyr)

dt <- select(archive_cases_dv_subset$DT,-percent_cli)
```

We can see how the omission of LOCF values changes the dataset; these LOCF
values use up space that may slow down performance.

```{r}
locf_included <- as_epi_archive(dt,compactify=FALSE)
locf_omitted <- as_epi_archive(dt,compactify=TRUE)

head(locf_included$DT)
head(locf_omitted$DT)

nrow(locf_included$DT)
nrow(locf_omitted$DT)
```

As we can see, performing 1,000 iterations of `dplyr::filter` is faster when the
LOCF values are omitted from the dataset we extract.

```{r}
# Performance of filtering iterated 1000 times
iterate_filter <- function(my_ea) {
  for (i in 1:1000) {
    filter(my_ea$DT,version >= as.Date("2020-01-01") + i)
  }
}

elapsed_time <- function(fx) c(system.time(fx))[[3]]

speed_test <- function(f,name) {
  data.frame(
    operation = name,
    locf=elapsed_time(f(locf_included)),
    no_locf=elapsed_time(f(locf_omitted))
  )
}

speed_test(iterate_filter,"filter_1000x")

```

```{r}
# Performance of as_of iterated 1,000 times
iterate_as_of <- function(my_ea) {
  for (i in 1:1000) {
    my_ea$as_of(min(my_ea$DT$time_value) + i - 1000)
  }
}

speeds <- speed_test(iterate_as_of,"as_of_1000x")

# Performance of 100 instantiations, not applying LOCF when instantiating
iterate_new_compactify_false <- function(my_ea) {
  for (i in 1:100) {
    epi_archive$new(my_ea$DT,compactify = FALSE)
  }
}

speeds <- rbind(speeds, speed_test(iterate_new_compactify_false,
                                   "new_compactify_false_100x"))

# Performance of 100 instantiations, applying LOCF when instantiating
iterate_new_compactify_true <- function(my_ea) {
  for (i in 1:100) {
    epi_archive$new(my_ea$DT,compactify = TRUE)
  }
}

speeds <- rbind(speeds, speed_test(iterate_new_compactify_true,
                                   "new_compactify_true_100x"))

# Performance of slide
slide_median <- function(my_ea) {
    my_ea$slide(median = median(case_rate_7d_av))
}

speeds <- rbind(speeds, speed_test(slide_median,"slide_median"))
speeds
```
We would also like to measure the speed of `epi_archive` methods. The most
noticeable time-saving is with dataset instantiation. Here is a detailed
performance comparison:

```{r}
library(tidyr)
speeds_tidy <- gather(speeds,key="is_locf",value="time_in_s",locf,no_locf)

library(ggplot2)

ggplot(speeds_tidy) +
  geom_bar(aes(x=is_locf,y=time_in_s,fill=operation),stat = "identity")
```

Given that it takes time to run `compactify`, it takes longer to instantiate
a new dataset when it is set to `TRUE` rather than `FALSE`. It also takes
longer to instantiate new datasets when the LOCF values haven't been removed.